# ğŸš€ é«˜çº§åŠŸèƒ½ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç» `@ldesign/upload` çš„é«˜çº§åŠŸèƒ½æ¨¡å—ã€‚

---

## ğŸ“‘ ç›®å½•

1. [æ–‡ä»¶å»é‡æ£€æµ‹](#æ–‡ä»¶å»é‡æ£€æµ‹)
2. [ä¸Šä¼ é€Ÿç‡é™æµ](#ä¸Šä¼ é€Ÿç‡é™æµ)
3. [WebWorker å¤„ç†æ± ](#webworker-å¤„ç†æ± )
4. [IndexedDB ç¦»çº¿ç¼“å­˜](#indexeddb-ç¦»çº¿ç¼“å­˜)
5. [MD5 å“ˆå¸Œè®¡ç®—](#md5-å“ˆå¸Œè®¡ç®—)
6. [å›¾ç‰‡æ¨¡ç³Šæ»¤é•œ](#å›¾ç‰‡æ¨¡ç³Šæ»¤é•œ)

---

## 1. æ–‡ä»¶å»é‡æ£€æµ‹

ä½¿ç”¨ `DuplicationDetector` é˜²æ­¢ç”¨æˆ·é‡å¤ä¸Šä¼ ç›¸åŒæ–‡ä»¶ã€‚

### åŸºç¡€ç”¨æ³•

```typescript
import { DuplicationDetector } from '@ldesign/upload'

// åˆ›å»ºæ£€æµ‹å™¨
const detector = new DuplicationDetector({
  useContentHash: true,      // ä½¿ç”¨å†…å®¹å“ˆå¸Œ(æ›´å‡†ç¡®ä½†è¾ƒæ…¢)
  maxCacheSize: 1000,         // æœ€å¤šç¼“å­˜ 1000 ä¸ªæ–‡ä»¶æŒ‡çº¹
  cacheDuration: 30           // ç¼“å­˜ 30 å¤©
})

// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦é‡å¤
const file = new File(['content'], 'test.txt')
const { isDuplicate, existingFile } = await detector.isDuplicate(file)

if (isDuplicate) {
  console.log('æ–‡ä»¶å·²ä¸Šä¼ è¿‡:', existingFile)
  // æç¤ºç”¨æˆ·æˆ–è‡ªåŠ¨è·³è¿‡
} else {
  // ä¸Šä¼ æ–‡ä»¶
  await uploadFile(file)
  
  // æ ‡è®°ä¸ºå·²ä¸Šä¼ 
  await detector.markAsUploaded(file)
}
```

### ä¸ Uploader é›†æˆ

```typescript
import { Uploader, DuplicationDetector } from '@ldesign/upload'

const detector = new DuplicationDetector()
const uploader = new Uploader({
  endpoint: '/api/upload',
  onFileAdded: async (fileItem) => {
    // æ£€æµ‹é‡å¤
    const { isDuplicate, existingFile } = await detector.isDuplicate(fileItem.file)
    
    if (isDuplicate) {
      console.warn('é‡å¤æ–‡ä»¶:', existingFile)
      uploader.removeFile(fileItem.id)
      alert(`æ–‡ä»¶ ${fileItem.file.name} å·²ç»ä¸Šä¼ è¿‡äº†`)
    }
  },
  onUploadSuccess: async (fileId) => {
    const file = uploader.getFile(fileId)
    if (file) {
      await detector.markAsUploaded(file.file)
    }
  }
})
```

### API å‚è€ƒ

```typescript
class DuplicationDetector {
  // æ£€æŸ¥æ˜¯å¦é‡å¤
  async isDuplicate(file: File): Promise<{
    isDuplicate: boolean
    existingFile?: FileFingerprint
  }>
  
  // è·å–æ–‡ä»¶æŒ‡çº¹
  async getFileFingerprint(file: File): Promise<string>
  
  // æ ‡è®°ä¸ºå·²ä¸Šä¼ 
  async markAsUploaded(file: File): Promise<void>
  
  // ä»ç¼“å­˜ç§»é™¤
  async removeFromCache(file: File): Promise<void>
  
  // è·å–ç»Ÿè®¡ä¿¡æ¯
  getStats(): {
    cacheSize: number
    maxCacheSize: number
    useContentHash: boolean
  }
  
  // æ¸…ç©ºç¼“å­˜
  clear(): void
}
```

---

## 2. ä¸Šä¼ é€Ÿç‡é™æµ

ä½¿ç”¨ `RateLimiter` æ§åˆ¶ä¸Šä¼ é€Ÿåº¦,é˜²æ­¢å ç”¨è¿‡å¤šå¸¦å®½ã€‚

### åŸºç¡€é™æµå™¨

```typescript
import { RateLimiter } from '@ldesign/upload'

// åˆ›å»ºé™æµå™¨: æœ€å¤§ 1MB/s
const limiter = new RateLimiter(1 * 1024 * 1024)

// åœ¨ä¸Šä¼ å‰è°ƒç”¨ throttle
async function uploadChunk(chunk: Blob) {
  await limiter.throttle(chunk.size)  // ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€
  
  // æ‰§è¡Œå®é™…ä¸Šä¼ 
  await fetch('/upload', {
    method: 'POST',
    body: chunk
  })
}

// åŠ¨æ€è°ƒæ•´é€Ÿç‡
limiter.setMaxBytesPerSecond(2 * 1024 * 1024) // æ”¹ä¸º 2MB/s

// è·å–å½“å‰é€Ÿç‡
console.log('å½“å‰é€Ÿç‡:', limiter.getCurrentRate(), 'bytes/s')

// ç¦ç”¨é™æµ
limiter.disable()
```

### è‡ªé€‚åº”é™æµå™¨

æ ¹æ®ç½‘ç»œçŠ¶å†µè‡ªåŠ¨è°ƒæ•´ä¸Šä¼ é€Ÿç‡:

```typescript
import { AdaptiveRateLimiter } from '@ldesign/upload'

const limiter = new AdaptiveRateLimiter(1 * 1024 * 1024) // åˆå§‹ 1MB/s

async function uploadWithAdaptive(chunk: Blob) {
  const startTime = Date.now()
  
  await limiter.throttle(chunk.size)
  
  try {
    await fetch('/upload', { method: 'POST', body: chunk })
    
    const latency = Date.now() - startTime
    limiter.recordSuccess(latency) // è®°å½•æˆåŠŸ(ä¼šè‡ªåŠ¨è°ƒæ•´é€Ÿç‡)
  } catch (error) {
    limiter.recordFailure() // è®°å½•å¤±è´¥(ä¼šé™ä½é€Ÿç‡)
    throw error
  }
}

// è·å–ç½‘ç»œè´¨é‡è¯„ä¼°
const quality = limiter.getNetworkQuality()
console.log('ç½‘ç»œè´¨é‡:', quality) // 'excellent' | 'good' | 'fair' | 'poor'
```

### ä¸ Uploader é›†æˆ

```typescript
import { Uploader, RateLimiter } from '@ldesign/upload'
import { HTTPAdapter } from '@ldesign/upload/adapters'

const limiter = new RateLimiter(2 * 1024 * 1024) // 2MB/s

// è‡ªå®šä¹‰é€‚é…å™¨,é›†æˆé™æµ
class ThrottledAdapter extends HTTPAdapter {
  async uploadChunk(chunk: Blob, index: number, total: number, fileId: string) {
    await limiter.throttle(chunk.size)
    return super.uploadChunk(chunk, index, total, fileId)
  }
}

const uploader = new Uploader({
  adapter: new ThrottledAdapter({ endpoint: '/api/upload' }),
  chunked: true
})
```

---

## 3. WebWorker å¤„ç†æ± 

ä½¿ç”¨ `WorkerPool` åœ¨åå°çº¿ç¨‹å¤„ç† CPU å¯†é›†å‹ä»»åŠ¡ã€‚

### åŸºç¡€ç”¨æ³•

```typescript
import { WorkerPool } from '@ldesign/upload'

// åˆ›å»º Worker æ± (è‡ªåŠ¨ä½¿ç”¨ CPU æ ¸å¿ƒæ•°)
const pool = new WorkerPool()
await pool.init()

// å‹ç¼©å›¾ç‰‡
const imageFile = new File([...], 'photo.jpg', { type: 'image/jpeg' })
const compressed = await pool.compressImage(imageFile, {
  quality: 0.8,
  maxWidth: 1920,
  maxHeight: 1080
})

// è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
const hash = await pool.calculateHash(imageFile)
console.log('SHA-256:', hash)

// è·å–ç»Ÿè®¡
console.log(pool.getStats())
// { totalWorkers: 4, availableWorkers: 4, busyWorkers: 0, queuedTasks: 0 }

// æ¸…ç†
pool.terminate()
```

### ä½¿ç”¨å…¨å±€å•ä¾‹

```typescript
import { getWorkerPool, terminateWorkerPool } from '@ldesign/upload'

// è·å–å…¨å±€æ± 
const pool = getWorkerPool()

// æ‰¹é‡å¤„ç†
const files = [...] // File[]
const results = await Promise.all(
  files.map(file => pool.compressImage(file, { quality: 0.7 }))
)

// åº”ç”¨å…³é—­æ—¶æ¸…ç†
window.addEventListener('beforeunload', () => {
  terminateWorkerPool()
})
```

### ä¸å›¾ç‰‡å¤„ç†é›†æˆ

```typescript
import { ImageProcessor, getWorkerPool } from '@ldesign/upload'

const processor = new ImageProcessor({
  compress: true,
  quality: 0.8
})

const pool = getWorkerPool()

// åœ¨ Worker ä¸­å‹ç¼©(ä¸é˜»å¡ä¸»çº¿ç¨‹)
async function processImageInBackground(file: File) {
  return await pool.compressImage(file, {
    quality: 0.8,
    maxWidth: 1920,
    maxHeight: 1080
  })
}

// æˆ–åœ¨ä¸»çº¿ç¨‹å‹ç¼©
async function processImageInMainThread(file: File) {
  return await processor.compress(file)
}
```

### è‡ªå®šä¹‰ä»»åŠ¡

```typescript
const pool = new WorkerPool()
await pool.init()

// æ‰§è¡Œè‡ªå®šä¹‰ä»»åŠ¡
const result = await pool.execute({
  id: crypto.randomUUID(),
  type: 'custom',
  data: { /* è‡ªå®šä¹‰æ•°æ® */ },
  priority: 2 // é«˜ä¼˜å…ˆçº§
})
```

---

## 4. IndexedDB ç¦»çº¿ç¼“å­˜

ä½¿ç”¨ `OfflineCache` å®ç°ç¦»çº¿ä¸Šä¼ å’Œæ–­ç‚¹ç»­ä¼ ã€‚

### åŸºç¡€ç”¨æ³•

```typescript
import { OfflineCache } from '@ldesign/upload'

// åˆ›å»ºç¼“å­˜
const cache = new OfflineCache({
  cacheDuration: 7,    // ç¼“å­˜ 7 å¤©
  maxCacheSize: 500    // æœ€å¤§ 500MB
})

await cache.init()

// ç¼“å­˜æ–‡ä»¶
const file = new File(['content'], 'document.pdf')
await cache.cacheFile('file-123', file)

// æ¢å¤æ–‡ä»¶
const cachedFile = await cache.retrieveFile('file-123')
if (cachedFile) {
  console.log('ä»ç¼“å­˜æ¢å¤:', cachedFile.name)
}

// ç¼“å­˜ä¸Šä¼ çŠ¶æ€
await cache.cacheUploadState({
  id: 'upload-123',
  fileId: 'file-123',
  status: 'uploading',
  progress: 45,
  uploadedChunks: [0, 1, 2],
  totalChunks: 10
})

// è·å–ç»Ÿè®¡
const stats = await cache.getStats()
console.log('ç¼“å­˜ä½¿ç”¨:', stats.cacheSize / 1024 / 1024, 'MB')
```

### å®ç°ç¦»çº¿ä¸Šä¼ é˜Ÿåˆ—

```typescript
import { Uploader, OfflineCache } from '@ldesign/upload'

const cache = new OfflineCache()
await cache.init()

const uploader = new Uploader({
  endpoint: '/api/upload',
  onFileAdded: async (fileItem) => {
    // ç¼“å­˜æ–‡ä»¶
    await cache.cacheFile(fileItem.id, fileItem.file)
  },
  onUploadProgress: async (event) => {
    // ä¿å­˜è¿›åº¦
    await cache.cacheUploadState({
      id: event.fileId,
      fileId: event.fileId,
      status: 'uploading',
      progress: event.progress,
      uploadedChunks: [],
      totalChunks: 0
    })
  },
  onUploadSuccess: async (fileId) => {
    // æ¸…ç†ç¼“å­˜
    await cache.deleteFile(fileId)
  }
})

// æ¢å¤æœªå®Œæˆçš„ä¸Šä¼ 
async function resumeUploads() {
  const pending = await cache.getPendingUploads()
  
  for (const upload of pending) {
    const file = await cache.retrieveFile(upload.fileId)
    if (file) {
      await uploader.addFile(file)
    }
  }
}

// åº”ç”¨å¯åŠ¨æ—¶æ¢å¤
resumeUploads()
```

### åˆ†ç‰‡ç¼“å­˜

```typescript
// ç¼“å­˜æ–‡ä»¶åˆ†ç‰‡
const chunks = splitFileIntoChunks(file, 5 * 1024 * 1024)

for (let i = 0; i < chunks.length; i++) {
  await cache.cacheChunk(fileId, i, chunks[i])
}

// ä¸Šä¼ åæ ‡è®°
await cache.markChunkUploaded(fileId, 0)

// è·å–æœªä¸Šä¼ çš„åˆ†ç‰‡
const allChunks = await cache.getFileChunks(fileId)
const pending = allChunks.filter(chunk => !chunk.uploaded)

// ç»§ç»­ä¸Šä¼ 
for (const chunk of pending) {
  await uploadChunk(chunk.data)
  await cache.markChunkUploaded(fileId, chunk.index)
}
```

---

## 5. MD5 å“ˆå¸Œè®¡ç®—

`ChunkManager` ç°å·²é›†æˆ `@ldesign/crypto` è¿›è¡Œ MD5 å“ˆå¸Œè®¡ç®—ã€‚

### åŸºç¡€ç”¨æ³•

```typescript
import { ChunkManager } from '@ldesign/upload'

const chunkManager = new ChunkManager()

// è®¡ç®—åˆ†ç‰‡å“ˆå¸Œ
const file = new File(['content'], 'test.txt')
const chunks = chunkManager.splitFile(file)
const hash = await chunkManager.calculateChunkHash(chunks[0])
console.log('åˆ†ç‰‡ MD5:', hash)

// è®¡ç®—æ•´ä¸ªæ–‡ä»¶å“ˆå¸Œ
const fileHash = await chunkManager.calculateFileHash(file)
console.log('æ–‡ä»¶ MD5:', fileHash)
```

### ç”¨äºç§’ä¼ åŠŸèƒ½

```typescript
import { Uploader, ChunkManager } from '@ldesign/upload'

const chunkManager = new ChunkManager()

const uploader = new Uploader({
  endpoint: '/api/upload',
  onFileAdded: async (fileItem) => {
    // è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
    const hash = await chunkManager.calculateFileHash(fileItem.file)
    
    // æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²æœ‰è¯¥æ–‡ä»¶
    const response = await fetch(`/api/check-hash?hash=${hash}`)
    const { exists, url } = await response.json()
    
    if (exists) {
      console.log('æ–‡ä»¶å·²å­˜åœ¨,ç§’ä¼ æˆåŠŸ!')
      // ç›´æ¥æ ‡è®°ä¸ºæˆåŠŸ
      uploader.getFile(fileItem.id)!.status = 'success'
    }
  }
})
```

### éªŒè¯åˆ†ç‰‡å®Œæ•´æ€§

```typescript
// ä¸Šä¼ å‰è®¡ç®—å“ˆå¸Œ
const chunk = chunks[0]
const originalHash = await chunkManager.calculateChunkHash(chunk)

// ä¸Šä¼ åˆ†ç‰‡
await uploadChunk(chunk, originalHash)

// æœåŠ¡å™¨ç«¯éªŒè¯
// POST /upload/chunk
// Body: { chunk, index, hash: originalHash }
```

---

## 6. å›¾ç‰‡æ¨¡ç³Šæ»¤é•œ

`ImageProcessor` ç°å·²å®Œæ•´å®ç°æ¨¡ç³Šæ»¤é•œã€‚

### åŸºç¡€ç”¨æ³•

```typescript
import { ImageProcessor } from '@ldesign/upload'

const processor = new ImageProcessor()

// åº”ç”¨æ¨¡ç³Šæ»¤é•œ
const file = new File([...], 'photo.jpg', { type: 'image/jpeg' })
const blurred = await processor.applyFilters(file, [
  { type: 'blur', value: 10 } // æ¨¡ç³ŠåŠå¾„ 10px
])

// ç»„åˆå¤šä¸ªæ»¤é•œ
const processed = await processor.applyFilters(file, [
  { type: 'blur', value: 5 },
  { type: 'brightness', value: 0.2 },
  { type: 'contrast', value: 1.2 }
])
```

### æ‰€æœ‰å¯ç”¨æ»¤é•œ

```typescript
// ç°åº¦
{ type: 'grayscale' }

// æ¨¡ç³Š
{ type: 'blur', value: 10 }

// äº®åº¦ (-1 åˆ° 1)
{ type: 'brightness', value: 0.3 }

// å¯¹æ¯”åº¦ (0 åˆ° 2)
{ type: 'contrast', value: 1.5 }

// å¤å¤è‰²è°ƒ
{ type: 'sepia' }

// è‡ªå®šä¹‰æ»¤é•œ
{ 
  type: 'custom',
  customFilter: (canvas) => {
    const ctx = canvas.getContext('2d')
    // è‡ªå®šä¹‰å¤„ç†
  }
}
```

---

## ğŸ¯ å®Œæ•´ç¤ºä¾‹

ç»¼åˆä½¿ç”¨æ‰€æœ‰é«˜çº§åŠŸèƒ½:

```typescript
import {
  Uploader,
  DuplicationDetector,
  AdaptiveRateLimiter,
  getWorkerPool,
  OfflineCache,
  ChunkManager
} from '@ldesign/upload'

// åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
const detector = new DuplicationDetector({ useContentHash: true })
const limiter = new AdaptiveRateLimiter(2 * 1024 * 1024)
const workerPool = getWorkerPool()
const cache = new OfflineCache()
const chunkManager = new ChunkManager()

await Promise.all([
  workerPool.init(),
  cache.init()
])

// åˆ›å»ºä¸Šä¼ å™¨
const uploader = new Uploader({
  endpoint: '/api/upload',
  chunked: true,
  
  onFileAdded: async (fileItem) => {
    const file = fileItem.file
    
    // 1. æ£€æµ‹é‡å¤
    const { isDuplicate } = await detector.isDuplicate(file)
    if (isDuplicate) {
      uploader.removeFile(fileItem.id)
      alert('æ–‡ä»¶å·²ä¸Šä¼ è¿‡')
      return
    }
    
    // 2. å¦‚æœæ˜¯å›¾ç‰‡,åœ¨ Worker ä¸­å‹ç¼©
    if (file.type.startsWith('image/')) {
      const compressed = await workerPool.compressImage(file, {
        quality: 0.8,
        maxWidth: 1920
      })
      // æ›¿æ¢æ–‡ä»¶
      fileItem.file = new File([compressed], file.name, { type: file.type })
    }
    
    // 3. è®¡ç®—å“ˆå¸Œ(ç”¨äºç§’ä¼ )
    const hash = await chunkManager.calculateFileHash(file)
    const checkResult = await fetch(`/api/check?hash=${hash}`)
    const { exists } = await checkResult.json()
    
    if (exists) {
      console.log('ç§’ä¼ æˆåŠŸ!')
      return
    }
    
    // 4. ç¼“å­˜åˆ° IndexedDB
    await cache.cacheFile(fileItem.id, file)
  },
  
  onUploadProgress: async (event) => {
    // é™æµ
    await limiter.throttle(event.uploadedSize)
    
    // ç¼“å­˜è¿›åº¦
    await cache.cacheUploadState({
      id: event.fileId,
      fileId: event.fileId,
      status: 'uploading',
      progress: event.progress,
      uploadedChunks: [],
      totalChunks: 0
    })
  },
  
  onUploadSuccess: async (fileId) => {
    // æ ‡è®°ä¸ºå·²ä¸Šä¼ 
    const file = uploader.getFile(fileId)
    if (file) {
      await detector.markAsUploaded(file.file)
    }
    
    // æ¸…ç†ç¼“å­˜
    await cache.deleteFile(fileId)
  }
})

// åº”ç”¨å¯åŠ¨æ—¶æ¢å¤æœªå®Œæˆçš„ä¸Šä¼ 
async function init() {
  const pending = await cache.getPendingUploads()
  for (const upload of pending) {
    const file = await cache.retrieveFile(upload.fileId)
    if (file) {
      await uploader.addFile(file)
    }
  }
}

init()
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨ Worker æ± å¤„ç†å›¾ç‰‡** - é¿å…é˜»å¡ä¸»çº¿ç¨‹
2. **å¯ç”¨è‡ªé€‚åº”é™æµ** - æ ¹æ®ç½‘ç»œçŠ¶å†µè‡ªåŠ¨è°ƒæ•´
3. **ä½¿ç”¨å…ƒæ•°æ®æŒ‡çº¹** - å¿«é€Ÿæ£€æµ‹é‡å¤(éå…³é”®åœºæ™¯)
4. **åˆç†è®¾ç½®ç¼“å­˜æ—¶é•¿** - å¹³è¡¡å­˜å‚¨ç©ºé—´å’Œæ¢å¤èƒ½åŠ›
5. **åˆ†ç‰‡å¤§å°ä¼˜åŒ–** - ä½¿ç”¨ `RateLimiter.getOptimalChunkSize()`

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### Worker åˆå§‹åŒ–å¤±è´¥
```typescript
// æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
if (!window.Worker) {
  console.warn('æµè§ˆå™¨ä¸æ”¯æŒ Web Workers')
  // å›é€€åˆ°ä¸»çº¿ç¨‹å¤„ç†
}
```

### IndexedDB é…é¢è¶…é™
```typescript
const stats = await cache.getStats()
if (stats.cacheSize > stats.maxCacheSize * 0.9) {
  // æ¸…ç†è¿‡æœŸç¼“å­˜
  await cache.clearExpiredCache()
}
```

### å†…å®¹å“ˆå¸Œè®¡ç®—è€—æ—¶
```typescript
// å¯¹äºå¤§æ–‡ä»¶,ä½¿ç”¨å…ƒæ•°æ®æŒ‡çº¹
const detector = new DuplicationDetector({
  useContentHash: file.size < 10 * 1024 * 1024 // ä»…å°äº10MBä½¿ç”¨å†…å®¹å“ˆå¸Œ
})
```

---

**Built with â¤ï¸ by LDesign Team**
